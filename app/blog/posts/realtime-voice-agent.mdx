---
title: "Voice Agents with OpenAI's Realtime API"
publishedAt: "2025-03-19"
summary: "Explore the architecture of OpenAI's Realtime API and a practical approach to building dynamic, low-latency voice agents with advanced conversational capabilities."
---

## Achieving Natural Voice Interactions

Traditional voice systems often struggle with latency, unnatural turn-taking, and limited contextual understanding. Creating AI agents that can engage in fluid, real-time conversations requires a shift away from standard request-response models. OpenAI's Realtime API aims to address this by providing a dedicated infrastructure for low-latency, stateful voice interactions, enabling the development of more natural and responsive voice agents.

## Understanding the OpenAI API Architecture

The Realtime API introduces a different paradigm compared to typical HTTP-based AI APIs. Its architecture is specifically designed for the demands of real-time voice communication:

**1. Stateful WebSocket Foundation**

Instead of stateless requests, the API operates over a persistent WebSocket connection, enabling continuous, bidirectional communication:

- **Persistent Connection:** Maintains the conversational state across multiple turns without needing to resend context repeatedly.
- **Event-Driven Protocol:** Interactions are managed through a defined set of client-sent and server-sent events, allowing for precise control over the conversation flow.
- **Bidirectional Streaming:** Allows audio data and control events to flow in both directions simultaneously.

**2. Integrated Audio Handling**

The API natively processes audio streams, simplifying the typical voice pipeline:

- **Direct Audio Input/Output:** Supports formats like 16-bit, 24kHz PCM and G.711, processing raw audio data directly.
- **Built-in Voice Activity Detection (VAD):** Automatically detects pauses in user speech to determine when they have likely finished speaking.
- **Interruption Capability:** Allows the user to interrupt the AI's response naturally, mirroring human conversation dynamics.

**3. Sophisticated Conversation Management**

Beyond basic audio streaming, the API provides tools for managing the conversation effectively:

- **Phrase Endpointing:** Intelligently determines the appropriate moment for the AI to respond based on VAD and context.
- **Context Management:** The persistent connection inherently manages the short-term conversational context.
- **Tool Integration (Function Calling):** Supports defining and triggering external tools or functions based on the conversation.

## A Practical Implementation Approach

To leverage the Realtime API effectively, we developed a system incorporating WebRTC for connection management, a composable agent architecture, and integrated web search capabilities. Here's how the key components work:

**1. WebRTC for Realtime Communication**

We chose WebRTC to establish the connection, handling both the audio stream and the data channel for API events within a unified framework:

```typescript
export async function createRealtimeConnection(
  EPHEMERAL_KEY: string,
  audioElement: RefObject<HTMLAudioElement | null>
): Promise<{ pc: RTCPeerConnection; dc: RTCDataChannel }> {
  const pc = new RTCPeerConnection();

  pc.ontrack = (e) => {
    if (audioElement.current) {
      audioElement.current.srcObject = e.streams[0];
    }
  };

  const ms = await navigator.mediaDevices.getUserMedia({ audio: true });
  pc.addTrack(ms.getTracks()[0]);

  const dc = pc.createDataChannel("oai-events");

  const offer = await pc.createOffer();
  await pc.setLocalDescription(offer);

  const sdpResponse = await fetch(/* ... OpenAI endpoint ... */, {
    method: "POST",
    body: offer.sdp,
    headers: { Authorization: `Bearer ${EPHEMERAL_KEY}` },
  });
  const answerSdp = await sdpResponse.text();
  await pc.setRemoteDescription({ type: "answer", sdp: answerSdp });

  return { pc, dc };
}
```

<small>
  _This approach integrates media streaming and event handling within the WebRTC
  framework._
</small>

**2. Secure Session Management**

To securely initiate connections, an ephemeral API key is generated server-side for each session and passed to the client. This avoids exposing the primary API key in the frontend code.

**3. Composable Agent Design**

We implemented a modular architecture where different "agents" handle specific tasks (e.g., greeting, searching). This allows for specialization and easier management:

```typescript
greeter.downstreamAgents = [search];
search.downstreamAgents = [greeter];

const agents = injectTransferTools([greeter, search]);

export default agents;
```

Each agent has its own instructions and potentially specific tools:

```typescript
const search: AgentConfig = {
  name: "search",
  instructions: searchWebPrompt,
  tools: [webSearchTool()],
  toolLogic: {
    webSearch: async ({ query }: { query: string }) => {
      const response = await fetch("/api/search", {
        /* ... */
      });
      const result = await response.json();
      return result;
    },
  },
};
```

<small>
  _This modularity allows for adding or modifying capabilities without altering a single
  monolithic agent definition._
</small>

**4. Dynamic Agent Transfers**

A key feature is the ability to transfer the conversation between agents based on context. This is enabled by automatically injecting a `transferAgents` tool into agents that have downstream connections defined. This tool allows the AI to understand the available specialized agents and decide when to hand off the conversation, passing along relevant context.

**5. Core Voice Agent Component**

The central UI component manages the connection lifecycle, session state, event handling, and interaction logic:

```typescript
function VoiceAgent() {
  const connectToRealtime = async () => {};

  const updateSession = (shouldTriggerResponse: boolean = false) => {
    const sessionUpdateEvent = {
      type: "session.update",
      session: { tools: currentAgent?.tools || [] },
    };
    sendClientEvent(sessionUpdateEvent);
  };

  const handleServerEvent = (event: any) => {};
}
```

<small>
  _This component orchestrates the user-facing experience and interacts with the
  underlying connection and agent logic._
</small>

## Key Design Considerations

Building this system involved addressing several practical challenges inherent in real-time voice AI:

**1. Minimizing Perceived Latency**

- **Direct WebRTC:** Using WebRTC potentially reduces intermediary hops.
- **Client-Side Buffering:** Managing audio playback smooths out network jitter.
- **Efficient Session Initiation:** Using server-generated ephemeral keys streamlines the handshake.

**2. Ensuring Natural Conversation Flow**

- **Server-Side VAD Tuning:** Adjusting VAD parameters is crucial for responsiveness.
- **Clear User Feedback:** Displaying transcripts and status helps users.
- **Interruption Handling:** Relying on the API's native support is key.

**3. Agent Specialization and Handover**

- **Modular Design:** Separating agent concerns simplifies management.
- **Contextual Transfers:** The `transferAgents` tool enables intelligent routing.
- **Defined Roles:** Clear instructions improve predictability.

## Integrating Web Search

A specific tool implemented provides agents with the ability to access external information via a backend endpoint:

```typescript
export async function POST(req: Request) {
  const { model, query } = await req.json();

  const response = await openai.responses.create({
    model,
    tools: [{ type: "web_search_preview" }],
    input: query,
  });

  return NextResponse.json(response);
}
```

<small>
  _This allows agents like the "search" agent to fulfill information requests beyond their
  initial training data._
</small>

## Conclusion

Developing effective voice agents with OpenAI's Realtime API requires careful consideration of the connection layer, agent architecture, tool integration, and user interface. Our approach demonstrates one way to combine these elements:

1.  **Connection:** Utilizing WebRTC for integrated media and event handling.
2.  **Architecture:** Employing a composable agent design for modularity and specialization.
3.  **Tools:** Implementing dynamic agent transfers and external capabilities like web search.
4.  **Interaction:** Managing session state and user feedback through a central UI component.

This architecture provides a foundation for building sophisticated, responsive voice applications. Future work could involve expanding the range of specialized agents, refining VAD and interruption handling based on user testing, and exploring more advanced context management strategies. The Realtime API offers powerful capabilities, and thoughtful system design is key to unlocking its full potential for natural conversational AI.
